# 关闭防火墙
systemctl stop firewalld
systemctl disable firewalld
# 生成密钥
ssh-keygen -t rsa
# 获得公钥
cat /root/.ssh/id_rsa.pub

ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDDh6cyFvbm4rNP2xriPRjpiwiTxLxLLY3yCg+PXcjvQAJqjLbOl3tyQXdZv2BylF0TzB5kf3Le+4poykodzZKSQ/evFaT8AuZgUzeTi3xwnJuTbmWkCTQeqvtp00bYPQgQ9ox05CZh8EBA96X2GDI6GE1YhKR63j+kgAfHxIL6XilRpoKaoevP50+yVO28wzyVNIbbB5qmtMK1tRB1S5Stym2lNOuYVGC7gY/zQ3mt6veVHEGLsk6pdEJd9h2IFI3M7lxwjUljeFddUgoQN5fRaP0pXhks9kqVhZrnAIGBnDLmtrXFAKqTL4riNqdYqBmD160pX0uA29Nl+Wht4S6/ root@zcy-2020211418-0001
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDrztiUbCeqDdkDsjLYjYq6Yp2ZMN29jX0JOR8ziFvM891hUPhsgwSg+We66uvbDpPehBKgpnWYszfyL91wc7OMDAy7uvJkJ00ITxU80Oaf5ZtouiH/jU9BnnK6h9rVQSgwGfAO2ZX0fxU19Ys4VgW2JKe/FTIa+aKD+tRU7mhghXxhVwR7uKF8wQ5wrRZJxnS8ALNbJnMuHSrv2PCEl0TBaqU4KGKB012npfOXNPUblTn5qLG8UnOy7NMjDq4ti2iUkyNL/BvCVKiiW7SA4g5CXu+J9ZDOH4HqNm6m7xCUUQ9Ik4V6+GyX+TO2jIE2sUtmuMB3BZMCVqq0zRKb8PeL root@zcy-2020211418-0002
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0FYzmoSyytgqTBIiJIyPQ0Si/wQiNZGR19EgkQG9bis+iXRrVe4Zyw5VTLLop+GsovoyZaRRGrrFuR5erRUk/zqo+j1z4YpC8GRuNuG+ZF0VjtaLd3Hhnj5naGyPIXLizRcXq1EhNFRY4bPSu+pJh+/QX0iLH0OwQBAz/MEEOdLXwHXfcqNyltVu3V36YMahO23FzK+Bpr0Qvqowe7Ai1FIWS73mq8CgezyQcAIPBG056YW8sqOYVEgZ3SqPaG5fj/867LVdNS6gn3XziWStCRCo0K4d/6tpQtbGu0zroL1V+BdSwpSuvY8dpb6qhPyiAFXVcbQUWzX/HxBu4rEtX root@zcy-2020211418-0003
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDD2Ib57Rh8o2prGZPAOxFnBrsKjoP1UzXCC6ipUA89xYqyeJ7UvVEemcXqV+tu4oeP5f6rVJDacIU8S37Qx+X/2u+oxJm6tUFGSY8qiQR598BaOGhSYpCNhEX5qF6icbEuTS/WI1IWvMkx99Lt4ch8aIaqDlOk3ooGjXTHtUeDn8G3co9wb3ZXWLGas9PAiirVinPU1Wc6/G0gr1C2/TrVik1IMtzpopc6Gdlc2MkFjeo5JeJ4WwvUiX737u20W5fjc5sh1nftVroFTb2ZiDjJONB4UCfY+XoI4Awr+xn+VUSHb2o47sps4qB+4+3v0me37xsK0dt7fFD6VqhcSukB root@zcy-2020211418-0004
# 复制公钥
vim /root/.ssh/authorized_keys
# 查看内网IP并配置hosts
vim /etc/hosts
|节点| 公网IP | 内网IP |
|-----|---------|---------|
|node1|120.46.162.37|192.168.0.210|
|node2|120.46.172.218|192.168.0.113|
|node3|120.46.186.200|192.168.0.237|
|node4|120.46.170.78|192.168.0.88|
**可能需要删掉localhost一行，api访问datanode会返回localhost导致文件写入失败**

# 检测节点间是否能无密访问
![[Pasted image 20230314221523.png]]
# 安装 OpenJDK
- 拷贝
cp OpenJDK8U-jdk_aarch64_linux_openj9_8u292b10_openj9-0.26.0.tar /usr/lib/jvm/
- 分发到其他节点
scp OpenJDK8U-jdk_aarch64_linux_openj9_8u292b10_openj9-0.26.0.tar root@node1:/usr/lib/jvm/ 
scp OpenJDK8U-jdk_aarch64_linux_openj9_8u292b10_openj9-0.26.0.tar root@node2:/usr/lib/jvm/ 
scp OpenJDK8U-jdk_aarch64_linux_openj9_8u292b10_openj9-0.26.0.tar root@node3:/usr/lib/jvm/
- 解压
tar -vxf OpenJDK8U-jdk_aarch64_linux_openj9_8u292b10_openj9-0.26.0.tar
- 编辑配置
vim /etc/profile
添加export JAVA_HOME=/usr/lib/jvm/jdk8u292-b10
- 确认Java版本
source /etc/profile
java -version
![[Pasted image 20230314223620.png]]
# 安装hadoop
- 复制安装包
cp -r hadoop-2.7.7 /home/modules/
cd /home/modules/
![[Pasted image 20230314224032.png]]
- 配置hadoop环境变量
vim /home/modules/hadoop-2.7.7/etc/hadoop/hadoop-env.sh
加入
export JAVA_HOME=/usr/lib/jvm/jdk8u292-b10
- 配置hadoop/core-site.xml
vim /home/modules/hadoop-2.7.7/etc/hadoop/core-site.xml
```
<configuration>
<property>
 <name>fs.obs.readahead.inputstream.enabled</name>
 <value>true</value>
</property>
<property>
 <name>fs.obs.buffer.max.range</name>
 <value>6291456</value>
</property>
<property>
 <name>fs.obs.buffer.part.size</name>
 <value>2097152</value>
</property>
<property>
 <name>fs.obs.threads.read.core</name>
 <value>500</value>
</property>
<property>
 <name>fs.obs.threads.read.max</name>
 <value>1000</value>
</property>
<property>
 <name>fs.obs.write.buffer.size</name>
 <value>8192</value>
</property>
<property>
 <name>fs.obs.read.buffer.size</name>
 <value>8192</value>
</property>
<property>
 <name>fs.obs.connection.maximum</name>
 <value>1000</value>
</property>
<property>
 <name>fs.defaultFS</name>
 <value>hdfs://node1:8020</value>
</property>
<property>
 <name>hadoop.tmp.dir</name>
 <value>/home/modules/hadoop-2.8.3/tmp</value>
</property>
<property>
 <name>fs.obs.access.key</name>
 <value>YDCXRXVJPIF1UFAXG0N6</value>
</property>
<property>
 <name>fs.obs.secret.key</name>
 <value>AUmr4m5tcy0BHl9EiwlEardPYyTWD6hAmQDBsfbf</value>
</property>
<property>
 <name>fs.obs.endpoint</name>
 <value>obs.cn-north-4.myhuaweicloud.com</value>
</property>
<property>
 <name>fs.obs.buffer.dir</name>
 <value>/home/modules/data/buf</value>
</property>
<property>
 <name>fs.obs.impl</name>
 <value>org.apache.hadoop.fs.obs.OBSFileSystem</value>
</property>
<property>
 <name>fs.obs.connection.ssl.enabled</name>
 <value>false</value>
</property>
<property>
 <name>fs.obs.fast.upload</name>
 <value>true</value>
</property>
<property>
 <name>fs.obs.socket.send.buffer</name>
 <value>65536</value>
</property>
<property>
 <name>fs.obs.socket.recv.buffer</name>
 <value>65536</value>
</property>
<property>
 <name>fs.obs.max.total.tasks</name>
 <value>20</value>
</property>
<property>
 <name>fs.obs.threads.max</name>
<value>20</value>
</property>
</configuration>
```
fs.defaultFS、fs.obs.access.key、fs.obs.secret.key、fs.obs.endpoint 需根据实际情况修改
- 配置 hdfs-site.xml
vim /home/modules/hadoop-2.7.7/etc/hadoop/hdfs-site.xml
```
<configuration>
<property>
 <name>dfs.replication</name>
 <value>3</value>
</property>
<property>
 <name>dfs.namenode.secondary.http-address</name>
 <value>node1:50090</value>
</property>
<property>
 <name>dfs.namenode.secondary.https-address</name>
 <value>node1:50091</value>
</property>
</configuration>
```
- 配置 yarn-site.xml
vim /home/modules/hadoop-2.7.7/etc/hadoop/yarn-site.xml
```
<configuration>
<property>
 <name>yarn.nodemanager.local-dirs</name>
<value>/home/nm/localdir</value>
</property>
<property>
 <name>yarn.nodemanager.resource.memory-mb</name>
 <value>28672</value>
</property>
 <property>
 <name>yarn.scheduler.minimum-allocation-mb</name>
 <value>3072</value>
</property>
 <property>
 <name>yarn.scheduler.maximum-allocation-mb</name>
 <value>28672</value> 
</property>
 <property>
 <name>yarn.nodemanager.resource.cpu-vcores</name>
 <value>38</value>
</property>
 <property>
 <name>yarn.scheduler.maximum-allocation-vcores</name>
 <value>38</value>
</property>
<property>
 <name>yarn.nodemanager.aux-services</name>
 <value>mapreduce_shuffle</value>
</property>
<property>
 <name>yarn.resourcemanager.hostname</name>
 <value>node1</value>
</property>
<property>
 <name>yarn.log-aggregation-enable</name>
 <value>true</value>
</property>
<property>
 <name>yarn.log-aggregation.retain-seconds</name>
 <value>106800</value>
</property>
<property>
 <name>yarn.nodemanager.vmem-check-enabled</name>
 <value>false</value>
 <description>Whether virtual memory limits will be enforced for containers</description>
</property>
<property>
 <name>yarn.nodemanager.vmem-pmem-ratio</name>
 <value>4</value>
 <description>Ratio between virtual memory to physical memory when setting memory limits for 
containers</description>
</property>
<property>
 <name>yarn.resourcemanager.scheduler.class</name><value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</value>
 </property>
<property>
 <name>yarn.log.server.url</name>
 <value>http://node1:19888/jobhistory/logs</value>
</property>
</configuration>
```
- 配置 mapred-sit.xml
cd /home/modules/hadoop-2.7.7/etc/hadoop/
mv mapred-site.xml.template mapred-site.xml
vim /home/modules/hadoop-2.7.7/etc/hadoop/mapred-site.xml
```
<configuration>
<property>
 <name>mapreduce.framework.name</name>
 <value>yarn</value>
</property>
<property>
 <name>mapreduce.jobhistory.address</name>
 <value>node1:10020</value>
</property>
<property>
 <name>mapreduce.jobhistory.webapp.address</name>
 <value>node1:19888</value>
</property>
<property>
 <name>mapred.task.timeout</name>
 <value>1800000</value>
</property>
</configuration>
```
- 配置 slaves
vim /home/modules/hadoop-2.7.7/etc/hadoop/slaves
```
node2
node3
node4
```
- 分发 hadoop 包到其余节点
在其余 3 个节点上创建目标文件夹 mkdir /home/modules/
分发
scp -r /home/modules/hadoop-2.7.7 root@node2:/home/modules/
scp -r /home/modules/hadoop-2.7.7 root@node3:/home/modules/
scp -r /home/modules/hadoop-2.7.7 root@node4:/home/modules/
- 配置环境变量
所有节点 vim /etc/profile
```
export HADOOP_HOME=/home/modules/hadoop-2.7.7
export PATH=$JAVA_HOME/bin:$PATH
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
export HADOOP_CLASSPATH=/home/modules/hadoop-2.7.7/share/hadoop/tools/lib/*:$HADOOP_CLASSPATH
```
source /etc/profile
chmod -R 777 /home/modules/hadoop-2.7.7
在上传安装包的节点执行下列命令 hadoop namenode -format
启动 hadoop：start-all.sh
![[Pasted image 20230314233249.png]]
![[Pasted image 20230314233314.png]]

# 结果
![[Pasted image 20230315030943.png]]
![[Pasted image 20230315031033.png]]

![[Pasted image 20230315031225.png]]
